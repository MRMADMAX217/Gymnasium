BLACKJACK TRAINING REPORT

1. OBJECTIVE
The goal was to implement a Reinforcement Learning (RL) agent using Q-Learning to play the game of Blackjack (Blackjack-v1).

2. IMPLEMENTATION DETAILS
- Algorithm: Q-Learning with epsilon-greedy exploration.
- State Space: Tuple of (Player Sum, Dealer Showing Card, Usable Ace).
- Action Space: Discrete(2) - Hit (1) or Stick (0).
- Hyperparameters:
  - Episodes: 500,000
  - Learning Rate: 0.001 (tuned down from 0.01)
  - Discount Factor: 0.95

3. PROBLEMS FACED
- Instability: Initially, a learning rate of 0.01 caused the training error to oscillate and increase over time. The agent was over-correcting its Q-values.
  - Solution: Lowered learning rate to 0.001, which stabilized the learning curve.
- High Variance: The standard deviation of rewards remained high (~0.96) even after extensive training.
  - Conclusion: This was determined to be inherent to the stochastic nature of Blackjack (luck of the draw) rather than a flaw in the agent.

4. PERFORMANCE METRICS
- Achieved Win Rate: ~44.2%
- Ideal Win Rate: ~43-44% (for a standard "Basic Strategy" without card counting).

5. CONCLUSION
We successfully trained an agent that plays at a near-optimal level. The achieved win rate of 44.2% matches the theoretical maximum for a player using a fixed strategy against the house, confirming the agent has "solved" the game within the constraints of the environment.
