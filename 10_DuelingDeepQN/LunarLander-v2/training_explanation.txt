Dueling DQN Training Explanation - LunarLander-v2

1. The Dueling DQN Algorithm
----------------------------
Dueling Deep Q-Networks (Dueling DQN) improve upon standard DQN by explicitly separating the representation of state values and action advantages.

Architecture:
Instead of outputting Q-values directly from the main neural network, the Dueling DQN splits into two streams after the initial convolutional or fully connected layers:
1. Value Stream V(s): Estimates the value of being in state 's', regardless of the action taken. This tells us "how good is it to be here?".
2. Advantage Stream A(s, a): Estimates the advantage of taking action 'a' in state 's' compared to other actions. This tells us "how much better is this action than the average?".

Aggregation:
The final Q-value is calculated by combining these two streams:
Q(s, a) = V(s) + (A(s, a) - mean(A(s, a)))

Subtracting the mean advantage increases stability by forcing the advantage function to have zero mean, ensuring V(s) represents the true value of the state.

2. Training Process
-------------------
- Episodes: 1000
- Optimizer: Adam (Learning Rate: 0.0005)
- Loss Function: Mean Squared Error (MSE)
- Replay Buffer: Stores up to 100,000 transitions to break correlation between consecutive samples.
- Target Network: Updated every 4 episodes to stabilize training targets.
- Epsilon Greedy: Starts at 1.0 (100% random) and decays to 0.01 to balance exploration and exploitation.

3. Reward Calculation (LunarLander-v2/v3)
-----------------------------------------
The goal is to land the module on the landing pad (coordinates 0,0). The reward system is granular:

- Distance: Reward given for moving from the top of the screen to the landing pad (approx +100 to +140 points).
- Speed: Negative reward for moving away from the pad or moving too fast.
- Tilt: Negative reward for tilting (the lander should stay upright).
- Leg Contact: +10 points for each leg that touches the ground.
- Main Engine Usage: -0.3 points per frame (incentivizes efficient fuel use).
- Side Engine Usage: -0.03 points per frame.
- Outcome:
    - Safe Landing: +100 points (resting on the ground with zero speed).
    - Crash: -100 points (body touches ground or moves out of bounds).

4. Performance Comparison
-------------------------
Ideal Reward (Solved):
- The environment is considered solved when the agent achieves an average reward of 200 points or higher over 100 consecutive episodes.
- A perfect run (efficient landing, no crashing) can achieve 250+ points.

Achieved Reward:
- Early Training: The agent will likely crash often, resulting in negative rewards (e.g., -100 to -300).
- Mid Training: As it learns to hover and stay upright, rewards should stabilize around 0 to +100.
- Final Performance: A well-trained Dueling DQN should consistently achieve >200 points, landing safely and efficiently.

Note: Check the generated dueling_dqn_training.png and console logs for the specific rewards achieved in your run.

5. Evaluation Results
---------------------
The trained model was evaluated over 5 episodes.
Average Reward: 173.80

Episode Breakdown:
- Episode 1: -72.97
- Episode 2: 246.49
- Episode 3: 232.24
- Episode 4: 251.84
- Episode 5: 211.40

Conclusion:
The agent demonstrates strong performance, achieving >200 points in 4 out of 5 episodes. The low score in Episode 1 brings down the average, but the high scores indicate the agent has learned to land successfully and efficiently.
