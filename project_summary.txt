PROJECT REPORT: BLACKJACK AGENT OPTIMIZATION

1. INITIAL OBSERVATION
When I first ran the Q-learning agent on the Blackjack environment (3_BlackJack.py), I observed two main issues:
- Instability: The "Training Error" plot showed an increasing trend over time. This meant the agent wasn't converging to a stable policy; instead, its estimates were getting worse or oscillating.
- High Variance: The standard deviation of rewards was high, and the win rate was stuck around 42%.

2. ACTIONS TAKEN

Step 1: Stabilizing the Learning Process
I hypothesized that the learning rate (0.01) was too high, causing the agent to over-correct its Q-values.
- Action: I reduced the learning_rate to 0.001.
- Result: This successfully stabilized the training. The error plot flipped to a decreasing trend, confirming the agent was learning effectively.

Step 2: Improving Visualization
I needed a better way to track progress than just transient plots.
- Action: I modified the script to save the training plots to a file (training_results.png) automatically. This allowed me to keep a record of the agent's learning curve.

Step 3: Maximizing Performance
To see if I could improve the win rate and reduce variance, I decided to train the agent for much longer.
- Action: I increased the training episodes from 100,000 to 500,000 and adjusted the exploration decay to match this longer duration.
- Result: The win rate improved to 44.2%, which is very close to the theoretical maximum for Blackjack.

3. CONCLUSION ON VARIANCE
I attempted to reduce the standard deviation (0.96) through this extended training. However, I observed that it remained high.
- Conclusion: I concluded that this high variance is inherent to the game of Blackjack itself (due to the luck of the draw) and is not a flaw in the agent. The agent is playing consistently, but the game outcomes are naturally volatile.

FINAL STATUS
The agent is now stable, achieves a near-optimal win rate (44.2%), and I have verified its learning progress through the generated plots.

