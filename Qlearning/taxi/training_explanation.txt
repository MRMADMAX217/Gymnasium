Taxi-v3 Training Explanation
============================

This directory contains the code and results for training a Q-learning agent on the Taxi-v3 environment.

Files
-----
taxi_train.py: The main training script.
taxi_eval.py: Script to evaluate the trained agent.
taxi_q_values.pkl: The saved Q-table (pickle file) containing the learned values.
taxi_training_results.png: A plot showing the cumulative rewards, episode lengths, and training error.
taxi_summary.txt: Summary of the environment and task.

Training Details (taxi_train.py)
--------------------------------
The agent was trained using the Q-learning algorithm.

Hyperparameters:
- Episodes: 20,000
- Learning Rate (Alpha): 0.01
- Discount Factor (Gamma): 0.95
- Epsilon (Exploration Rate): Starts at 1.0 and decays linearly over the first 10,000 episodes to a minimum of 0.1.

Process:
1. Initialization: A Q-table is initialized with zeros.
2. Loop: For each episode:
   - The agent chooses an action using an epsilon-greedy strategy.
   - The agent takes the action, observes the new state and reward.
   - The Q-value is updated using the Bellman equation.
   - Epsilon is decayed.
3. Output:
   - The final Q-table is saved to taxi_q_values.pkl.
   - A plot of training metrics is saved to taxi_training_results.png.

Results
-------
The goal of the Taxi environment is to pick up a passenger and drop them off at the destination in the fewest steps possible.

Standard Benchmark:
The environment is considered "solved" when the agent achieves an average reward of 9.7 or higher over 100 consecutive trials.

Evaluation:
Run "python taxi_eval.py" to check the current performance.
- Average Reward: The primary metric. Should be >= 9.7.
- Success Rate: Percentage of episodes where the agent successfully delivered the passenger (positive reward).
