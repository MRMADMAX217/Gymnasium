TAXI-V3 TRAINING REPORT
=======================

1. OBJECTIVE
The goal was to train an RL agent to autonomously navigate a 5x5 grid, pick up a passenger from one of 4 locations, and drop them off at a destination (Taxi-v3).

2. IMPLEMENTATION DETAILS
- Algorithm: Q-Learning (adapted from the Blackjack implementation).
- State Space: Discrete(500) - Combinations of taxi row, col, passenger location, and destination.
- Action Space: Discrete(6) - South, North, East, West, Pickup, Dropoff.
- Hyperparameters:
  - Episodes: 20,000 (significantly fewer than Blackjack due to faster convergence).
  - Learning Rate: 0.01 (higher than Blackjack).
  - Discount Factor: 0.95

3. PROBLEMS FACED
- Metric Definition: Unlike Blackjack (Win/Loss), Taxi is a continuous task where every step costs -1 point.
  - Solution: We defined a "Win" as achieving a total reward > 0 for the episode, which implies the agent successfully delivered the passenger without taking excessive steps.
- Adaptation: We had to adjust the hyperparameters (fewer episodes, higher learning rate) because the Taxi environment is deterministic and has a smaller, more structured state space compared to Blackjack.

4. PERFORMANCE METRICS
- Achieved Success Rate: 91.4%
- Achieved Average Reward: -9.65
- Ideal Success Rate: ~100% (Deterministic environment)
- Solved Benchmark: Average Reward >= 9.7

5. CONCLUSION
The agent achieved a 91.4% success rate in 20,000 episodes, demonstrating it has learned the core logic. However, the average reward of -9.65 is significantly below the "solved" threshold of 9.7. This indicates that while the agent delivers the passenger, it takes too many steps or incurs penalties (likely -10 for illegal moves) along the way. Further tuning of the learning rate or epsilon decay is needed to optimize efficiency.
