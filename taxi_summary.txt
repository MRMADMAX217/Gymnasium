TAXI-V3 TRAINING REPORT

1. OBJECTIVE
The goal was to train an RL agent to autonomously navigate a 5x5 grid, pick up a passenger from one of 4 locations, and drop them off at a destination (Taxi-v3).

2. IMPLEMENTATION DETAILS
- Algorithm: Q-Learning (adapted from the Blackjack implementation).
- State Space: Discrete(500) - Combinations of taxi row, col, passenger location, and destination.
- Action Space: Discrete(6) - South, North, East, West, Pickup, Dropoff.
- Hyperparameters:
  - Episodes: 20,000 (significantly fewer than Blackjack due to faster convergence).
  - Learning Rate: 0.01 (higher than Blackjack).
  - Discount Factor: 0.95

3. PROBLEMS FACED
- Metric Definition: Unlike Blackjack (Win/Loss), Taxi is a continuous task where every step costs -1 point.
  - Solution: We defined a "Win" as achieving a total reward > 0 for the episode, which implies the agent successfully delivered the passenger without taking excessive steps.
- Adaptation: We had to adjust the hyperparameters (fewer episodes, higher learning rate) because the Taxi environment is deterministic and has a smaller, more structured state space compared to Blackjack.

4. PERFORMANCE METRICS
- Achieved Win Rate: 91.0%
- Ideal Win Rate: ~95-98% (Since Taxi-v3 is a deterministic logic puzzle, a perfect agent should solve it every time).

5. CONCLUSION
The agent achieved a 91% success rate in just 20,000 episodes, demonstrating it has effectively learned the pickup and drop-off logic. While 91% is a strong result, the gap to the ideal 95-98% suggests that with slightly more training or fine-tuning of the exploration decay, the agent could achieve perfection.
