Johnnycode FrozenLake Training Explanation
==========================================

This directory contains the code and results for training a Q-learning agent on the FrozenLake-v1 environment (4x4, Non-Slippery).

Files
-----
frozen_lake_q_train.py: The training script.
frozen_lake_q_eval.py: The evaluation script.
frozen_lake8x8.pkl: The saved Q-table (Note: Filename inherited from original script, but map is 4x4).
frozen_lake8x8.png: Plot of cumulative rewards.

Training Details
----------------
The agent was trained using Q-learning on the deterministic (non-slippery) 4x4 map.

Hyperparameters:
- Episodes: 50,000
- Learning Rate (Alpha): 0.9
- Discount Factor (Gamma): 0.9
- Epsilon Decay: 0.00002 per episode (Reaches 0 at episode 50,000)

Results
-------
Since the environment is non-slippery (deterministic), the agent should ideally be able to find a safe path to the goal every time.

Performance Metrics:
- Achieved Win Rate (Eval): 77.80%
- Ideal Win Rate: 100% (Deterministic environment)

Conclusion:
The agent achieved a 77.8% win rate, a significant improvement over the previous 59.3%. Increasing the training episodes and adjusting the exploration decay helped the agent learn a better policy. Further tuning (e.g., even more episodes or a different decay schedule) could likely push this to 100%.

Problems Faced & Solutions
--------------------------
1. Suboptimal Convergence (Win Rate ~59%):
   - Problem: Initially, the agent was stuck at a moderate win rate despite a high learning rate (0.9).
   - Hypothesis: I suspected the high learning rate might be causing instability, so we lowered it to 0.1.
   - Result: Performance drastically dropped to ~19%. This revealed that for a deterministic environment like this, a high learning rate is actually beneficial as it allows the agent to quickly commit to good paths.

2. Insufficient Exploration:
   - Problem: The agent wasn't finding the optimal path consistently.
   - Solution: We reverted the learning rate to 0.9 but significantly increased the training duration (from 20,000 to 50,000 episodes) and slowed down the epsilon decay (from 0.0001 to 0.00002).
   - Result: This allowed the agent to explore more thoroughly before exploiting, raising the win rate to 77.8%.
